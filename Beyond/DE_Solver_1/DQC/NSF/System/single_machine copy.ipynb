{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import datetime\n",
    "now=datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits=2\n",
    "dev=qml.device('default.qubit', wires=num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary condition \n",
    "coefii = 1000\n",
    "\n",
    "X_0 = 0  #del\n",
    "Y_0 = 0     #w\n",
    "X_0_v = -1/coefii\n",
    "Y_0_v = 383.9911/coefii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential equation parameters\n",
    "ws = 376.9911\n",
    "\n",
    "# ws = 0.376\n",
    "# coefii = ws/4\n",
    "K1 = 5/coefii\n",
    "K2 = 10/coefii\n",
    "K3 = 1.7/coefii\n",
    "ws = ws/coefii\n",
    "\n",
    "def F_E1(X_prime, X,Y_prime,Y, t):       # DE, works with numpy arrays\n",
    "    return (X_prime+ws-Y)\n",
    "    # return X_prime+ws-Y \n",
    "\n",
    "def F_E2(X_prime, X,Y_prime,Y, t):       # DE, works with numpy arrays\n",
    "    # return Y_prime\n",
    "    return Y_prime-K1+K2*np.sin(coefii*X)+K3*(-ws+Y)*coefii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential equation's exact solution - for comparison\n",
    "def X_t(t):\n",
    "    # return X_0_v*np.cos(t)+(3*X_0_v+2*Y_0_v)*np.sin(t)\n",
    "    return 0\n",
    "\n",
    "def Y_t(t):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.random.uniform(0,3,size=150,requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding layer\n",
    "def embedding(x,wires):\n",
    "    qml.RY(x, wires=wires[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ham():\n",
    "    obs=[]\n",
    "    for j in range(num_qubits):\n",
    "        obs.append(qml.PauliX(j))\n",
    "        for k in range(j):\n",
    "            obs.append(qml.PauliZ(j)@qml.PauliZ(k))\n",
    "    coeffs=np.random.uniform(-1,1,len(obs))*10\n",
    "    qml.Hamiltonian(coeffs, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ansastz layer\n",
    "def layer(theta):\n",
    "    \n",
    "    # Apply Hamiltonian matrix\n",
    "    Ham()\n",
    "    \n",
    "    # Apply H gate\n",
    "    qml.Hadamard(0)\n",
    "    \n",
    "    # rotations on qubit 1\n",
    "    qml.RY(theta,wires=1)\n",
    "    \n",
    "    # CNOT\n",
    "    qml.CNOT(wires=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, diff_method=\"backprop\", interface=\"autograd\")\n",
    "def quantum_net(theta,x):\n",
    "    \n",
    "    # encode data\n",
    "    embedding(x,wires=range(num_qubits))\n",
    "    \n",
    "    # parameterized circuit layers\n",
    "    for v in theta: # (for lool along with the first dimension)\n",
    "        # print(v)\n",
    "        # Ham()\n",
    "        layer(v)\n",
    "    \n",
    "    qml.Hadamard(0)\n",
    "    \n",
    "    # return qml.expval(qml.PauliZ(0)),qml.expval(qml.PauliZ(1))\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H────────────────╭●──H──────╭●──H──────╭●──H──────╭●──H─┤  <Z>\n",
      "1: ──RY(0.00)──RY(M0)─╰X──RY(M1)─╰X──RY(M2)─╰X──RY(M3)─╰X────┤     \n"
     ]
    }
   ],
   "source": [
    "num_layers=4\n",
    "theta1=np.random.uniform(0,2*pi,size=(num_layers,num_qubits-1),requires_grad=True)\n",
    "theta2=np.random.uniform(0,2*pi,size=(num_layers,num_qubits-1),requires_grad=True)\n",
    "# theta = np.load('my_theta1.npy')\n",
    "print(qml.draw(quantum_net)(theta1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the classical layer   #TODO\n",
    "def classical_quantum_net(theta,w,t):\n",
    "    r1=quantum_net(theta,t)[0]\n",
    "    # r2=quantum_net(theta,x)[1]\n",
    "    # return w[0]+w[1]*r1+w[2]*r1**2+w[3]*r2+w[4]*r2**2\n",
    "    return w[0]+w[1]*r1+w[2]*r1**2\n",
    "    # r1=quantum_net(theta,x)\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_quantum_net_prime(theta,w,t):\n",
    "    r1=quantum_net(theta,t)[0]\n",
    "    r1_prime = qml.grad(quantum_net,argnum=1)(theta,np.tensor(t)) \n",
    "    # r2=quantum_net(theta,x)[1]\n",
    "    # return w[0]+w[1]*r1+w[2]*r1**2+w[3]*r2+w[4]*r2**2\n",
    "    return w[1]*r1_prime+2*w[2]*r1_prime*r1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def square_loss(labels,predictions):\n",
    "#     loss=0\n",
    "#     for l,p in zip(labels,predictions):\n",
    "#         loss=loss+(l-p)**2\n",
    "#     loss=loss/len(labels)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loss boundary\n",
    "# def loss_b(U_predict):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cost(theta1,theta2,w1,w2,t):\n",
    "\n",
    "    # loss inner points\n",
    "    loss_i = 0\n",
    "\n",
    "    X_t_pred = np.array([classical_quantum_net(theta=theta1,w=w1,t=point) for point in t])\n",
    "    X_t_prime = np.array([classical_quantum_net_prime(theta=theta1,w=w1,t=point) for point in t])\n",
    "    \n",
    "    Y_t_pred = np.array([classical_quantum_net(theta=theta2,w=w2,t=point) for point in t])\n",
    "    Y_t_prime = np.array([classical_quantum_net_prime(theta=theta2,w=w2,t=point) for point in t])\n",
    "\n",
    "    loss_i += np.mean(F_E1(X=X_t_pred,Y=Y_t_pred,X_prime=X_t_prime,Y_prime=Y_t_prime,t=t)**2)\n",
    "    loss_i += np.mean(F_E2(X=X_t_pred,Y=Y_t_pred,X_prime=X_t_prime,Y_prime=Y_t_prime,t=t)**2)\n",
    "\n",
    "    # loss boundary points\n",
    "    loss_b = 0\n",
    "    X_0_pred = classical_quantum_net(theta1,w1,0)\n",
    "    Y_0_pred = classical_quantum_net(theta2,w2,0)\n",
    "\n",
    "    # print(U_0)\n",
    "    loss_b += (X_0_v-X_0_pred)**2+(Y_0_v-Y_0_pred)**2\n",
    "    return 0.2*loss_b + 0.8*loss_i\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.zeros(3,requires_grad=True)\n",
    "w2=np.zeros(3,requires_grad=True)\n",
    "opt = AdamOptimizer(0.05, beta1=0.9, beta2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26 13:35:12.870954\n"
     ]
    }
   ],
   "source": [
    "start=now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Cost: 0.4769216767951436\n",
      "Epoch: 2 | Cost: 0.267925314063195\n",
      "Epoch: 3 | Cost: 0.12372832609843419\n",
      "Epoch: 4 | Cost: 0.05574576156572551\n",
      "Epoch: 5 | Cost: 0.05936035285338701\n",
      "Epoch: 6 | Cost: 0.09960685137320832\n",
      "Epoch: 7 | Cost: 0.13066279254313284\n",
      "Epoch: 8 | Cost: 0.1270601757287432\n",
      "Epoch: 9 | Cost: 0.0970834746364493\n",
      "Epoch: 10 | Cost: 0.06159190078976477\n",
      "Epoch: 11 | Cost: 0.03530185060545668\n",
      "Epoch: 12 | Cost: 0.02557369392973092\n",
      "Epoch: 13 | Cost: 0.029579379745954662\n",
      "Epoch: 14 | Cost: 0.04023912149944217\n",
      "Epoch: 15 | Cost: 0.0505142712158701\n",
      "Epoch: 16 | Cost: 0.05598741222061958\n",
      "Epoch: 17 | Cost: 0.056566531601915014\n",
      "Epoch: 18 | Cost: 0.05206454843781293\n",
      "Epoch: 19 | Cost: 0.04468325500753871\n",
      "Epoch: 20 | Cost: 0.03687396223969962\n",
      "Epoch: 21 | Cost: 0.030264407326721688\n",
      "Epoch: 22 | Cost: 0.026199536174735197\n",
      "Epoch: 23 | Cost: 0.025116650876396775\n",
      "Epoch: 24 | Cost: 0.02668487370743032\n",
      "Epoch: 25 | Cost: 0.02948437865382432\n",
      "Epoch: 26 | Cost: 0.032036744299331725\n",
      "Epoch: 27 | Cost: 0.03258414114552175\n",
      "Epoch: 28 | Cost: 0.03203613906234528\n",
      "Epoch: 29 | Cost: 0.02964779147253488\n",
      "Epoch: 30 | Cost: 0.02634667218886924\n",
      "Epoch: 31 | Cost: 0.023675176616007754\n",
      "Epoch: 32 | Cost: 0.022139814979606147\n",
      "Epoch: 33 | Cost: 0.021040506745471685\n",
      "Epoch: 34 | Cost: 0.02103454355044075\n",
      "Epoch: 35 | Cost: 0.021296184684287914\n",
      "Epoch: 36 | Cost: 0.021767199262678966\n",
      "Epoch: 37 | Cost: 0.022046116432501924\n",
      "Epoch: 38 | Cost: 0.021893033325794772\n",
      "Epoch: 39 | Cost: 0.02112494285424153\n",
      "Epoch: 40 | Cost: 0.019861434730512145\n",
      "Epoch: 41 | Cost: 0.018336673053091425\n",
      "Epoch: 42 | Cost: 0.016681048423669045\n",
      "Epoch: 43 | Cost: 0.015101624805386248\n",
      "Epoch: 44 | Cost: 0.013709657793575681\n",
      "Epoch: 45 | Cost: 0.012597275135737933\n",
      "Epoch: 46 | Cost: 0.01192255043826777\n",
      "Epoch: 47 | Cost: 0.011221232379724989\n",
      "Epoch: 48 | Cost: 0.010844840334068103\n",
      "Epoch: 49 | Cost: 0.010405102190328442\n",
      "Epoch: 50 | Cost: 0.010017788379999417\n",
      "Epoch: 51 | Cost: 0.009507405858963666\n",
      "Epoch: 52 | Cost: 0.00891613532365607\n",
      "Epoch: 53 | Cost: 0.008319799644646056\n",
      "Epoch: 54 | Cost: 0.007683837009022493\n",
      "Epoch: 55 | Cost: 0.007135898898457816\n",
      "Epoch: 56 | Cost: 0.006668144850626877\n",
      "Epoch: 57 | Cost: 0.006372334776826787\n",
      "Epoch: 58 | Cost: 0.006070029458620723\n",
      "Epoch: 59 | Cost: 0.005796224785897952\n",
      "Epoch: 60 | Cost: 0.005555822182296519\n",
      "Epoch: 61 | Cost: 0.00530499003317787\n",
      "Epoch: 62 | Cost: 0.005023854142944501\n",
      "Epoch: 63 | Cost: 0.004715227084402244\n",
      "Epoch: 64 | Cost: 0.004416195978694591\n",
      "Epoch: 65 | Cost: 0.00411471774688624\n",
      "Epoch: 66 | Cost: 0.0038596833827727155\n",
      "Epoch: 67 | Cost: 0.0035735011774612056\n",
      "Epoch: 68 | Cost: 0.0033993406559529446\n",
      "Epoch: 69 | Cost: 0.003211299722523235\n",
      "Epoch: 70 | Cost: 0.0030473099186554458\n",
      "Epoch: 71 | Cost: 0.0029158928069053687\n",
      "Epoch: 72 | Cost: 0.0027198999066436235\n",
      "Epoch: 73 | Cost: 0.0026290302091666845\n",
      "Epoch: 74 | Cost: 0.0023805699119342862\n",
      "Epoch: 75 | Cost: 0.0022613352781681324\n",
      "Epoch: 76 | Cost: 0.0020778755357756013\n",
      "Epoch: 77 | Cost: 0.0019490973040712871\n",
      "Epoch: 78 | Cost: 0.001834301608801122\n",
      "Epoch: 79 | Cost: 0.0017029031687828473\n",
      "Epoch: 80 | Cost: 0.0016397547472802357\n",
      "Epoch: 81 | Cost: 0.001526037951504355\n",
      "Epoch: 82 | Cost: 0.0014564029393963592\n",
      "Epoch: 83 | Cost: 0.00135321359126851\n",
      "Epoch: 84 | Cost: 0.0012638601778396307\n",
      "Epoch: 85 | Cost: 0.0011866633446973233\n",
      "Epoch: 86 | Cost: 0.0010958296282677202\n",
      "Epoch: 87 | Cost: 0.0010406271896645132\n",
      "Epoch: 88 | Cost: 0.0009678625541947366\n",
      "Epoch: 89 | Cost: 0.0008738666127300851\n",
      "Epoch: 90 | Cost: 0.0008836085997490375\n",
      "Epoch: 91 | Cost: 0.0008135718744844134\n",
      "Epoch: 92 | Cost: 0.0007227288209320467\n",
      "Epoch: 93 | Cost: 0.000736508043073322\n",
      "Epoch: 94 | Cost: 0.0006882912169818683\n",
      "Epoch: 95 | Cost: 0.0006041759212515184\n",
      "Epoch: 96 | Cost: 0.0005770448537859003\n",
      "Epoch: 97 | Cost: 0.0005720028607317251\n",
      "Epoch: 98 | Cost: 0.0005141580892560028\n",
      "Epoch: 99 | Cost: 0.00048488667332682845\n",
      "Epoch: 100 | Cost: 0.00044735646719096787\n",
      "Epoch: 101 | Cost: 0.00045238109729140417\n",
      "Epoch: 102 | Cost: 0.00041227472260644323\n",
      "Epoch: 103 | Cost: 0.0003941191826743012\n",
      "Epoch: 104 | Cost: 0.00035082815555744905\n",
      "Epoch: 105 | Cost: 0.00035096610088790304\n",
      "Epoch: 106 | Cost: 0.00034446012714590437\n",
      "Epoch: 107 | Cost: 0.00031923567780100045\n",
      "Epoch: 108 | Cost: 0.00029380717870418696\n",
      "Epoch: 109 | Cost: 0.00025828481148841456\n",
      "Epoch: 110 | Cost: 0.00026067873008000955\n",
      "Epoch: 111 | Cost: 0.00027840314506947275\n",
      "Epoch: 112 | Cost: 0.00026709569412126054\n",
      "Epoch: 113 | Cost: 0.00024201772830014428\n",
      "Epoch: 114 | Cost: 0.0002239602595551148\n",
      "Epoch: 115 | Cost: 0.00020226472593619095\n",
      "Epoch: 116 | Cost: 0.0001873297959983154\n",
      "Epoch: 117 | Cost: 0.000194934006526894\n",
      "Epoch: 118 | Cost: 0.00020021735464854063\n",
      "Epoch: 119 | Cost: 0.00019716297660360907\n",
      "Epoch: 120 | Cost: 0.00018565289847133167\n",
      "Epoch: 121 | Cost: 0.00017043168469206302\n",
      "Epoch: 122 | Cost: 0.00016565692713039603\n",
      "Epoch: 123 | Cost: 0.00016343280746233045\n",
      "Epoch: 124 | Cost: 0.00015063609059915814\n",
      "Epoch: 125 | Cost: 0.00013416469813122182\n",
      "Epoch: 126 | Cost: 0.0001347542084503283\n",
      "Epoch: 127 | Cost: 0.0001399472723321302\n",
      "Epoch: 128 | Cost: 0.00014022861856541222\n",
      "Epoch: 129 | Cost: 0.0001384416142828359\n",
      "Epoch: 130 | Cost: 0.00013675679608594053\n"
     ]
    }
   ],
   "source": [
    "epochs=130\n",
    "cost_max = 10000\n",
    "theta1_best = None\n",
    "theta2_best = None\n",
    "w1_best = None\n",
    "w2_best = None\n",
    "for e in range(1,epochs+1):\n",
    "    (theta1,theta2,w1,w2,_),_cost=opt.step_and_cost(cost,theta1,theta2,w1,w2,t)\n",
    "    if _cost<cost_max:\n",
    "        cost_max = _cost\n",
    "        theta1_best = theta1\n",
    "        theta2_best = theta2\n",
    "        w1_best = w1\n",
    "        w2_best = w2\n",
    "    # (theta,_),_cost=opt.step_and_cost(cost,theta,t)\n",
    "    if e==1 or e%1==0:\n",
    "        print(f'Epoch: {e} | Cost: {_cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013675679608594053\n"
     ]
    }
   ],
   "source": [
    "print(_cost)\n",
    "# t=np.random.uniform(0,1,size=50,requires_grad=False)\n",
    "pred_train_X=np.array([classical_quantum_net(theta1,w1,t) for t in t])\n",
    "pred_train_Y=np.array([classical_quantum_net(theta2,w2,t) for t in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      2\u001b[0m \u001b[39m# plt.scatter(t,X_t(t),facecolors='none', edgecolors='r', label='True - X_t')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mscatter(t,pred_train_X, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m,marker\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m,s\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPrediction - X_t\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "# plt.scatter(t,X_t(t),facecolors='none', edgecolors='r', label='True - X_t')\n",
    "plt.scatter(t,pred_train_X, color='red',marker=\"x\",s=4,label='Prediction - X_t')\n",
    "# plt.scatter(t,Y_t(t),facecolors='none', edgecolors='g', label='True - Y_t')\n",
    "# plt.scatter(t,pred_train_Y, color='green',marker=\"x\",s=4,label='Prediction - Y_t')\n",
    "plt.xlabel('x',fontsize=16)\n",
    "plt.ylabel('f(x)',fontsize=16)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(loc='upper center',bbox_to_anchor=(0.5,1.18),ncol=2,fontsize=16)\n",
    "# plt.savefig('1_measurement2_train.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
