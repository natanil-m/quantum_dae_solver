{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import datetime\n",
    "now=datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits=2\n",
    "dev=qml.device('default.qubit', wires=num_qubits)\n",
    "seed = 40\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.linspace(-10, 10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, diff_method=\"backprop\", interface=\"autograd\")\n",
    "def hepler(encoded_x,theta):\n",
    "    qml.QubitStateVector(encoded_x,wires=[i for i in range(num_qubits)])\n",
    "    # qml.RY(wires=0,phi=theta[0])\n",
    "    # qml.RX(wires=0,phi=theta[1])\n",
    "    qml.StronglyEntanglingLayers(weights=theta, wires=range(num_qubits))\n",
    "    return qml.probs(wires=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    vector_x = [1,x,x**2,x**3]\n",
    "    # vector_x = [1,x]\n",
    "    norm_x = np.linalg.norm(vector_x)\n",
    "    return vector_x/norm_x , norm_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(x,theta,w):\n",
    "    encoded_x,norm_x = encoder(x)\n",
    "    f_val = hepler(encoded_x,theta)\n",
    "    # rescale_coef = abs(w[0])\n",
    "    rescale_coef = w[0]\n",
    "    return (f_val[0]*(norm_x**2))*rescale_coef+w[1]\n",
    "    # print(qml.draw(hepler)(encoded_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(-1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_test(t):\n",
    "    return -5*t**2-20*t-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbackF(parameters):\n",
    "    # global current_loss\n",
    "\n",
    "    print(\"Current loss:\", cost(parameters)) \n",
    "\n",
    "\n",
    "def cost(params):\n",
    "    global theta\n",
    "    global w\n",
    "    # Reshape the flattened theta back to its original shape\n",
    "    theta_shape = theta.shape\n",
    "    w_shape = w.shape\n",
    "    w, theta_flat = params[:2], params[2:]\n",
    "    theta = theta_flat.reshape(theta_shape)\n",
    "   \n",
    "    loss = 0\n",
    "    for x in t:\n",
    "        loss += (f(x,theta,w)-f_test(x))**2/len(t)\n",
    "    \n",
    "    # print(w)\n",
    "    # print(theta)\n",
    "    # raise KeyError\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "Current loss: 16207.922093929765\n",
      "Current loss: 16197.725520419326\n",
      "Current loss: 16179.246043277552\n",
      "Current loss: 15994.51514444645\n",
      "Current loss: 15594.451115692971\n",
      "Current loss: 15492.79247467659\n",
      "Current loss: 15360.8612558235\n",
      "Current loss: 14851.00433832831\n",
      "Current loss: 14637.831405885438\n",
      "Current loss: 14391.777462560498\n",
      "Current loss: 14230.13509143207\n",
      "Current loss: 12731.668330296781\n",
      "Current loss: 10296.315826614189\n",
      "Current loss: 9531.755985110385\n",
      "Current loss: 8705.099170463716\n",
      "Current loss: 8151.27772614058\n",
      "Current loss: 8109.409700284486\n",
      "Current loss: 8069.41115475008\n",
      "Current loss: 8009.663842247239\n",
      "Current loss: 7923.305036481716\n",
      "Current loss: 7856.131583623562\n",
      "Current loss: 7809.549136451748\n",
      "Current loss: 7744.974159011244\n",
      "Current loss: 7628.797660002491\n",
      "Current loss: 7442.531512017625\n",
      "Current loss: 7306.659097167198\n",
      "Current loss: 7237.633053729939\n",
      "Current loss: 7156.096500486868\n",
      "Current loss: 7027.710264075367\n",
      "Current loss: 6972.401526762313\n",
      "Current loss: 6884.110149740685\n",
      "Current loss: 6839.198087789746\n",
      "Current loss: 6767.7479377722975\n",
      "Current loss: 6724.868355172458\n",
      "Current loss: 6692.283657373316\n",
      "Current loss: 6633.16910496771\n",
      "Current loss: 6552.682504706732\n",
      "Current loss: 6500.2173966239525\n",
      "Current loss: 6476.130170374472\n",
      "Current loss: 6440.47781364224\n",
      "Current loss: 6416.850633137279\n",
      "Current loss: 6384.294970706387\n",
      "Current loss: 6368.008710253462\n",
      "Current loss: 6344.580250046091\n",
      "Current loss: 6330.865560342573\n",
      "Current loss: 6306.152033342058\n",
      "Current loss: 6266.165381802424\n",
      "Current loss: 6215.5320402531015\n",
      "Current loss: 6201.19725972268\n",
      "Current loss: 6185.783847126038\n",
      "Current loss: 6171.909257657919\n",
      "Current loss: 6150.918158883855\n",
      "Current loss: 6123.373916949416\n",
      "Current loss: 6100.27977458441\n",
      "Current loss: 6063.849993565505\n",
      "Current loss: 6017.796149128727\n",
      "Current loss: 5996.314536012396\n",
      "Current loss: 5921.474669206926\n",
      "Current loss: 5894.087841916673\n",
      "Current loss: 5849.834481319492\n",
      "Current loss: 5783.762742977916\n",
      "Current loss: 5689.016365243382\n",
      "Current loss: 5603.929507820874\n",
      "Current loss: 5563.723205628213\n",
      "Current loss: 5520.2426835355545\n",
      "Current loss: 5505.849445628089\n",
      "Current loss: 5495.278638657529\n",
      "Current loss: 5486.962728958261\n",
      "Current loss: 5479.139633888406\n",
      "Current loss: 5471.836968503328\n",
      "Current loss: 5460.215893116228\n",
      "Current loss: 5441.762651035817\n",
      "Current loss: 5425.896678955112\n",
      "Current loss: 5412.987201232306\n",
      "Current loss: 5406.62880629386\n",
      "Current loss: 5402.181394062112\n",
      "Current loss: 5395.107469105617\n",
      "Current loss: 5387.2068767629635\n",
      "Current loss: 5376.688166575312\n",
      "Current loss: 5364.249426926661\n",
      "Current loss: 5355.469107337848\n",
      "Current loss: 5353.391932211316\n",
      "Current loss: 5350.135008325764\n",
      "Current loss: 5345.111480959833\n",
      "Current loss: 5340.278791121089\n",
      "Current loss: 5339.00463436619\n",
      "Current loss: 5337.0799559378975\n",
      "Current loss: 5334.6664802476125\n",
      "Current loss: 5333.20428039306\n",
      "Current loss: 5330.9904805694205\n",
      "Current loss: 5328.197239489303\n",
      "Current loss: 5325.315153504235\n",
      "Current loss: 5320.518124083534\n",
      "Current loss: 5316.193187864686\n",
      "Current loss: 5307.144483072394\n",
      "Current loss: 5305.428061793221\n",
      "Current loss: 5302.2010729730155\n",
      "Current loss: 5296.3074310067705\n",
      "Current loss: 5287.888030802689\n",
      "Current loss: 5284.727278991858\n",
      "Current loss: 5282.935891112896\n",
      "Current loss: 5282.164698260544\n",
      "Current loss: 5280.966517993209\n",
      "Current loss: 5279.285017851105\n",
      "Current loss: 5276.891475999477\n",
      "Current loss: 5272.980421407391\n",
      "Current loss: 5267.372147104497\n",
      "Current loss: 5266.120800144708\n",
      "Current loss: 5263.797218952277\n",
      "Current loss: 5259.630475078109\n",
      "Current loss: 5253.347177900512\n",
      "Current loss: 5247.461874196491\n",
      "Current loss: 5244.502743947259\n",
      "Current loss: 5242.455152303175\n",
      "Current loss: 5239.067472292394\n",
      "Current loss: 5235.032466177796\n",
      "Current loss: 5233.7872115727505\n",
      "Current loss: 5231.609684922696\n",
      "Current loss: 5227.791766639477\n",
      "Current loss: 5221.736685825191\n",
      "Current loss: 5214.210641082831\n",
      "Current loss: 5212.918319942066\n",
      "Current loss: 5210.548828830634\n",
      "Current loss: 5206.338975417635\n",
      "Current loss: 5200.036703715883\n",
      "Current loss: 5191.988828880408\n",
      "Current loss: 5189.3767267687335\n",
      "Current loss: 5185.62784199176\n",
      "Current loss: 5179.605639370652\n",
      "Current loss: 5172.360946496551\n",
      "Current loss: 5170.584712362501\n",
      "Current loss: 5161.498247437241\n",
      "Current loss: 5155.389896806592\n",
      "Current loss: 5147.1910356889975\n",
      "Current loss: 5141.772965031187\n",
      "Current loss: 5135.940585860444\n",
      "Current loss: 5130.754456842223\n",
      "Current loss: 5121.27988452241\n",
      "Current loss: 5112.87945462924\n",
      "Current loss: 5098.400401255756\n",
      "Current loss: 5078.605307191523\n",
      "Current loss: 5070.413970091738\n",
      "Current loss: 5058.633769108055\n",
      "Current loss: 5038.046484488908\n",
      "Current loss: 5018.109100974574\n",
      "Current loss: 5003.773814402905\n",
      "Current loss: 4992.362223668432\n",
      "Current loss: 4973.356288991018\n",
      "Current loss: 4951.933192085729\n",
      "Current loss: 4916.154270266994\n",
      "Current loss: 4876.88064767424\n",
      "Current loss: 4858.491555689253\n",
      "Current loss: 4850.385392648504\n",
      "Current loss: 4835.021847750952\n",
      "Current loss: 4805.878389274679\n",
      "Current loss: 4751.707221007487\n",
      "Current loss: 4666.9630494031835\n",
      "Current loss: 4543.704846352661\n",
      "Current loss: 4507.176756610479\n",
      "Current loss: 4482.344995527804\n",
      "Current loss: 4438.784219165546\n",
      "Current loss: 4376.718301796575\n",
      "Current loss: 4359.4853421514545\n",
      "Current loss: 4352.544392784346\n",
      "Current loss: 4347.556962468174\n",
      "Current loss: 4339.841186852467\n",
      "Current loss: 4330.892211959548\n",
      "Current loss: 4324.552280031639\n",
      "Current loss: 4318.857142977306\n",
      "Current loss: 4312.553156063435\n",
      "Current loss: 4309.501947219524\n",
      "Current loss: 4307.797029502884\n",
      "Current loss: 4306.492489677378\n",
      "Current loss: 4306.299138438374\n",
      "Current loss: 4306.021157035874\n",
      "Current loss: 4305.555937808408\n",
      "Current loss: 4304.716433690501\n",
      "Current loss: 4296.754184288436\n",
      "Current loss: 4294.049391934704\n",
      "Current loss: 4289.036582944632\n",
      "Current loss: 4279.467440852013\n",
      "Current loss: 4262.2251611092715\n",
      "Current loss: 4254.047046337422\n",
      "Current loss: 4252.582477568199\n",
      "Current loss: 4251.590926994046\n",
      "Current loss: 4251.461668664954\n",
      "Current loss: 4251.233498657378\n",
      "Current loss: 4250.840589224932\n",
      "Current loss: 4250.204203223328\n",
      "Current loss: 4249.265498301353\n",
      "Current loss: 4247.950122360402\n",
      "Current loss: 4245.711166357304\n",
      "Current loss: 4241.896408819932\n",
      "Current loss: 4235.680027054374\n",
      "Current loss: 4226.737803226416\n",
      "Current loss: 4218.13052284547\n",
      "Current loss: 4215.4190565039735\n",
      "Current loss: 4210.939443716346\n",
      "Current loss: 4204.53438372661\n",
      "Current loss: 4197.171295778627\n",
      "Current loss: 4190.480336658134\n",
      "Current loss: 4187.606909070386\n",
      "Current loss: 4184.380411946304\n",
      "Current loss: 4181.7629841155285\n",
      "Current loss: 4178.134872124814\n",
      "Current loss: 4176.921644318077\n",
      "Current loss: 4175.7643832181875\n",
      "Current loss: 4175.458086478437\n",
      "Current loss: 4175.259717703705\n",
      "Current loss: 4175.222157748683\n",
      "Current loss: 4175.196507803111\n",
      "Current loss: 4175.149489303583\n",
      "Current loss: 4175.060071967623\n",
      "Current loss: 4174.342280833311\n",
      "Current loss: 4171.206494219523\n",
      "Current loss: 4169.9353749736765\n",
      "Current loss: 4167.540781797272\n",
      "Current loss: 4163.640996715019\n",
      "Current loss: 4157.713151224461\n",
      "Current loss: 4148.40451081545\n",
      "Current loss: 4136.987347154221\n",
      "Current loss: 4128.524240498276\n",
      "Current loss: 4057.0267164992324\n",
      "Current loss: 4000.009599607156\n",
      "Current loss: 3986.7858678163575\n",
      "Current loss: 3983.405856406412\n",
      "Current loss: 3978.2018741498505\n",
      "Current loss: 3972.996311079424\n",
      "Current loss: 3969.2903823825673\n",
      "Current loss: 3963.0339264635727\n",
      "Current loss: 3953.9911785903487\n",
      "Current loss: 3949.0738741721766\n",
      "Current loss: 3946.0210727750955\n",
      "Current loss: 3943.3964647479156\n",
      "Current loss: 3941.7225063589353\n",
      "Current loss: 3939.608311823586\n",
      "Current loss: 3938.0183757892305\n",
      "Current loss: 3936.3877735094375\n",
      "Current loss: 3935.5687572024476\n",
      "Current loss: 3934.210735235187\n",
      "Current loss: 3932.111563841167\n",
      "Current loss: 3929.2782895927094\n",
      "Current loss: 3927.7822405123097\n",
      "Current loss: 3925.112966557038\n",
      "Current loss: 3920.2690536748714\n",
      "Current loss: 3912.0114813398586\n",
      "Current loss: 3901.1047008627224\n",
      "Current loss: 3887.7023175652553\n",
      "Current loss: 3882.831765523843\n",
      "Current loss: 3880.742839935362\n",
      "Current loss: 3878.522652154601\n",
      "Current loss: 3877.6431890152776\n",
      "Current loss: 3877.0241937410847\n",
      "Current loss: 3876.363367857896\n",
      "Current loss: 3875.326386549794\n",
      "Current loss: 3873.986239193819\n",
      "Current loss: 3873.4253320051525\n",
      "Current loss: 3872.3967608406756\n",
      "Current loss: 3870.544661544127\n",
      "Current loss: 3867.600077974731\n",
      "Current loss: 3864.8090328488756\n",
      "Current loss: 3863.365905270602\n",
      "Current loss: 3862.566413663282\n",
      "Current loss: 3861.8859740819535\n",
      "Current loss: 3860.9881193384713\n",
      "Current loss: 3859.8201172053155\n",
      "Current loss: 3858.9822385974985\n",
      "Current loss: 3858.5866196467987\n",
      "Current loss: 3858.128857650496\n",
      "Current loss: 3858.0041011214057\n",
      "Current loss: 3857.9068948193913\n",
      "Current loss: 3857.838974387575\n",
      "Current loss: 3857.7494353488896\n",
      "Current loss: 3857.6879563129733\n",
      "Current loss: 3857.6474976075688\n",
      "Current loss: 3857.586163829458\n",
      "Current loss: 3857.5002430620775\n",
      "Current loss: 3857.358151724152\n",
      "Current loss: 3857.128138618103\n",
      "Current loss: 3856.7236349913264\n",
      "Current loss: 3855.997219761293\n",
      "Current loss: 3854.686269502839\n",
      "Current loss: 3853.8676806751614\n",
      "Current loss: 3852.3701629108905\n",
      "Current loss: 3849.6697152265106\n",
      "Current loss: 3844.9119809893805\n",
      "Current loss: 3836.6683973201198\n",
      "Current loss: 3822.564653468284\n",
      "Current loss: 3801.9243354743867\n",
      "Current loss: 3791.075515577046\n",
      "Current loss: 3785.50840182336\n",
      "Current loss: 3775.958361169675\n",
      "Current loss: 3763.6554951008357\n",
      "Current loss: 3752.3865138572746\n",
      "Current loss: 3737.3954013036932\n",
      "Current loss: 3710.009310888086\n",
      "Current loss: 3665.4558337854382\n",
      "Current loss: 3588.9965690475406\n",
      "Current loss: 3573.3573844870216\n",
      "Current loss: 3545.2237693229963\n",
      "Current loss: 3523.543135211916\n",
      "Current loss: 3493.1671705199096\n",
      "Current loss: 3445.1170420845383\n",
      "Current loss: 3405.356041775244\n",
      "Current loss: 3344.767423172223\n",
      "Current loss: 3252.6331298324717\n",
      "Current loss: 3208.168944841006\n",
      "Current loss: 3173.980141316854\n",
      "Current loss: 3138.1797021288826\n",
      "Current loss: 3093.1020603564884\n",
      "Current loss: 3022.394465141038\n",
      "Current loss: 2980.784210352719\n",
      "Current loss: 2959.945499510434\n",
      "Current loss: 2939.5593049680083\n",
      "Current loss: 2903.024382537389\n",
      "Current loss: 2834.6594886674065\n",
      "Current loss: 2765.98584667099\n",
      "Current loss: 2668.0180194643654\n",
      "Current loss: 2503.189160868936\n",
      "Current loss: 2375.7768173901954\n",
      "Current loss: 2344.6217377309376\n",
      "Current loss: 2300.9361561178885\n",
      "Current loss: 2279.023124944215\n",
      "Current loss: 2254.9429740395753\n",
      "Current loss: 2212.588440862627\n",
      "Current loss: 2155.1017465996174\n",
      "Current loss: 2124.2292378090942\n",
      "Current loss: 2088.1496259429127\n",
      "Current loss: 2028.540202946691\n",
      "Current loss: 1959.1098711303077\n",
      "Current loss: 1932.9169174604906\n",
      "Current loss: 1900.047859194628\n",
      "Current loss: 1876.4441803863842\n",
      "Current loss: 1857.277174882774\n",
      "Current loss: 1848.4051335841436\n",
      "Current loss: 1842.228413882835\n",
      "Current loss: 1837.077032450759\n",
      "Current loss: 1828.5904062781296\n",
      "Current loss: 1823.4431425525836\n",
      "Current loss: 1818.1041189693833\n",
      "Current loss: 1811.6497704680824\n",
      "Current loss: 1800.9169000611457\n",
      "Current loss: 1786.90330943853\n",
      "Current loss: 1780.8553959290227\n",
      "Current loss: 1771.965548110695\n",
      "Current loss: 1766.1118585434688\n",
      "Current loss: 1757.101861837856\n",
      "Current loss: 1744.4512279074247\n",
      "Current loss: 1734.3922218111034\n",
      "Current loss: 1725.3067474829766\n",
      "Current loss: 1719.076005178235\n",
      "Current loss: 1714.6310604932016\n",
      "Current loss: 1712.9669581772896\n",
      "Current loss: 1710.1988203663873\n",
      "Current loss: 1705.6750094160145\n",
      "Current loss: 1702.9671298684268\n",
      "Current loss: 1701.2728317993399\n",
      "Current loss: 1698.9740079686196\n",
      "Current loss: 1697.3053478691784\n",
      "Current loss: 1694.8586078062153\n",
      "Current loss: 1690.497251918787\n",
      "Current loss: 1682.5861541689978\n",
      "Current loss: 1673.3565374070592\n",
      "Current loss: 1662.3971966260058\n",
      "Current loss: 1643.7584392708625\n",
      "Current loss: 1633.0024935236643\n",
      "Current loss: 1619.0477538340947\n",
      "Current loss: 1616.8893053149952\n",
      "Current loss: 1613.9216625884255\n",
      "Current loss: 1611.7884750632868\n",
      "Current loss: 1609.2758982216997\n",
      "Current loss: 1605.3190057528934\n",
      "Current loss: 1599.0823137934062\n",
      "Current loss: 1588.75243978942\n",
      "Current loss: 1575.9246743081799\n",
      "Current loss: 1571.3994774669702\n",
      "Current loss: 1563.8430706968522\n",
      "Current loss: 1552.658375750427\n",
      "Current loss: 1542.145308358008\n",
      "Current loss: 1537.4445736903986\n",
      "Current loss: 1529.8483594351305\n",
      "Current loss: 1519.3291482788131\n",
      "Current loss: 1511.0561283761667\n",
      "Current loss: 1506.5666615432401\n",
      "Current loss: 1504.743442157646\n",
      "Current loss: 1501.6677587589909\n",
      "Current loss: 1496.4405236415132\n",
      "Current loss: 1488.334355493901\n",
      "Current loss: 1485.3840742242555\n",
      "Current loss: 1480.5460286396094\n",
      "Current loss: 1473.8972421354374\n",
      "Current loss: 1469.6738135780988\n",
      "Current loss: 1467.8294118357271\n",
      "Current loss: 1464.592045646193\n",
      "Current loss: 1458.9208170690572\n",
      "Current loss: 1449.8150158924182\n",
      "Current loss: 1437.661099210232\n",
      "Current loss: 1434.8930164566111\n",
      "Current loss: 1429.9320575592606\n",
      "Current loss: 1421.429643148924\n",
      "Current loss: 1407.87480529887\n",
      "Current loss: 1384.9116823333159\n",
      "Current loss: 1374.8613579818216\n",
      "Current loss: 1368.7005864543016\n",
      "Current loss: 1364.8611971883868\n",
      "Current loss: 1358.6341233098776\n",
      "Current loss: 1348.8160878012561\n",
      "Current loss: 1339.1556423342645\n",
      "Current loss: 1328.9834872733381\n",
      "Current loss: 1323.071260842645\n",
      "Current loss: 1315.1806306843093\n",
      "Current loss: 1311.9696916479193\n",
      "Current loss: 1310.0503572951272\n",
      "Current loss: 1306.6748716938876\n",
      "Current loss: 1300.744029503266\n",
      "Current loss: 1292.4137099610136\n",
      "Current loss: 1289.2046060102589\n",
      "Current loss: 1288.7934291294398\n",
      "Current loss: 1288.025398049943\n",
      "Current loss: 1286.629533346018\n",
      "Current loss: 1284.2806685262938\n",
      "Current loss: 1280.9577031355266\n",
      "Current loss: 1278.7052854767092\n",
      "Current loss: 1278.0074413577759\n",
      "Current loss: 1276.8028502075574\n",
      "Current loss: 1275.0003368103216\n",
      "Current loss: 1274.1091123729968\n",
      "Current loss: 1273.8677752379979\n",
      "Current loss: 1273.469410417434\n",
      "Current loss: 1273.138040203739\n",
      "Current loss: 1272.7351354307912\n",
      "Current loss: 1272.3747120077642\n",
      "Current loss: 1272.1483773429575\n",
      "Current loss: 1272.043756971625\n",
      "Current loss: 1271.99077661176\n",
      "Current loss: 1271.9563133046681\n",
      "Current loss: 1271.9356775687213\n",
      "Current loss: 1271.9298116049279\n",
      "Current loss: 1271.9227655728923\n",
      "Current loss: 1271.9108393385632\n",
      "Current loss: 1271.8892151802738\n",
      "Current loss: 1271.8489662622321\n",
      "Current loss: 1271.7735805199927\n",
      "Current loss: 1271.6331514308856\n",
      "Current loss: 1271.3754240553421\n",
      "Current loss: 1270.9142230228708\n",
      "Current loss: 1270.118830697902\n",
      "Current loss: 1268.8046990594682\n",
      "Current loss: 1266.6610909233943\n",
      "Current loss: 1263.0058356609347\n",
      "Current loss: 1256.4673450195382\n",
      "Current loss: 1244.7336795961673\n",
      "Current loss: 1223.8596494630442\n",
      "Current loss: 1192.7943829063324\n",
      "Current loss: 1144.1285770312652\n",
      "Current loss: 1127.2327153197466\n",
      "Current loss: 1115.7255292158834\n",
      "Current loss: 1106.8922287096389\n",
      "Current loss: 1091.761378852843\n",
      "Current loss: 1072.818661678766\n",
      "Current loss: 1059.3478762082746\n",
      "Current loss: 1051.1817012485892\n",
      "Current loss: 1046.8440652593372\n",
      "Current loss: 1040.9961040365943\n",
      "Current loss: 1038.630442663631\n",
      "Current loss: 1037.3288005411632\n",
      "Current loss: 1036.9608108513617\n",
      "Current loss: 1036.564149645149\n",
      "Current loss: 1036.0785727154514\n",
      "Current loss: 1035.2997795203833\n",
      "Current loss: 1034.1446641351301\n",
      "Current loss: 1032.9497099250896\n",
      "Current loss: 1030.9024343171136\n",
      "Current loss: 1027.156389767941\n",
      "Current loss: 1020.596759426601\n",
      "Current loss: 1015.8539647537541\n",
      "Current loss: 1007.4430134368274\n",
      "Current loss: 997.9007178890029\n",
      "Current loss: 994.6698652542013\n",
      "Current loss: 991.1919483747523\n",
      "Current loss: 988.1754058756867\n",
      "Current loss: 986.8810092917508\n",
      "Current loss: 986.7083166553509\n",
      "Current loss: 986.5477619679151\n",
      "Current loss: 986.3329686087177\n",
      "Current loss: 986.2227577362992\n",
      "Current loss: 986.0369446112963\n",
      "Current loss: 985.7102109368325\n",
      "Current loss: 985.1222765186615\n",
      "Current loss: 984.0685116225727\n",
      "Current loss: 982.2292069075439\n",
      "Current loss: 979.153125701414\n",
      "Current loss: 978.1320116080674\n",
      "Current loss: 976.2024616374129\n",
      "Current loss: 972.7039370215513\n",
      "Current loss: 967.113365767628\n",
      "Current loss: 962.5667538734454\n",
      "Current loss: 958.8411263480147\n",
      "Current loss: 957.190144400924\n",
      "Current loss: 955.4933745710879\n",
      "Current loss: 954.6699309859607\n",
      "Current loss: 954.2756275450824\n",
      "Current loss: 954.1158657375452\n",
      "Current loss: 953.8649740812239\n",
      "Current loss: 953.4438538486139\n",
      "Current loss: 952.7014359813047\n",
      "Current loss: 951.3763983786165\n",
      "Current loss: 949.0487956118194\n",
      "Current loss: 945.3136280086037\n",
      "Current loss: 943.8465683307401\n",
      "Current loss: 941.1119039934051\n",
      "Current loss: 937.237989695316\n",
      "Current loss: 935.2006818527808\n",
      "Current loss: 934.6801207391912\n",
      "Current loss: 934.2856035619897\n",
      "Current loss: 934.0748039855031\n",
      "Current loss: 933.6866416194271\n",
      "Current loss: 932.975803613371\n",
      "Current loss: 931.7057410101995\n",
      "Current loss: 929.5512732635491\n",
      "Current loss: 926.6131657036152\n",
      "Current loss: 925.582505263679\n",
      "Current loss: 923.8415703936313\n",
      "Current loss: 921.0779752868222\n",
      "Current loss: 918.9775814992781\n",
      "Current loss: 917.9569004870831\n",
      "Current loss: 917.5339007400819\n",
      "Current loss: 917.1654982543238\n",
      "Current loss: 916.5974814568764\n",
      "Current loss: 915.6798671409508\n",
      "Current loss: 914.1193555419243\n",
      "Current loss: 911.5244758378234\n",
      "Current loss: 910.837378291973\n",
      "Current loss: 909.5433907619799\n",
      "Current loss: 907.3631223033723\n",
      "Current loss: 904.5952561897836\n",
      "Current loss: 903.4788664280806\n",
      "Current loss: 901.9062185886123\n",
      "Current loss: 901.1132269138194\n",
      "Current loss: 900.9877377043854\n",
      "Current loss: 900.7668598378328\n",
      "Current loss: 900.3575842170721\n",
      "Current loss: 899.5916707698753\n",
      "Current loss: 898.1721967182407\n",
      "Current loss: 895.6227769536592\n",
      "Current loss: 892.4589779981704\n",
      "Current loss: 888.4221819910907\n",
      "Current loss: 883.6985780752411\n",
      "Current loss: 879.5018634028205\n",
      "Current loss: 876.0486184239606\n",
      "Current loss: 874.9605813833737\n",
      "Current loss: 874.4370192452054\n",
      "Current loss: 874.0764245267591\n",
      "Current loss: 873.4842478809536\n",
      "Current loss: 872.5886324829809\n",
      "Current loss: 871.4052382784322\n",
      "Current loss: 869.3906094726777\n",
      "Current loss: 866.1565133003058\n",
      "Current loss: 864.3274738477802\n",
      "Current loss: 861.1452892147772\n",
      "Current loss: 856.0286053314895\n",
      "Current loss: 852.5074853636153\n",
      "Current loss: 847.3176781246735\n",
      "Current loss: 843.093998098211\n",
      "Current loss: 841.7313028630982\n",
      "Current loss: 840.7657213539419\n",
      "Current loss: 840.4716200311909\n",
      "Current loss: 840.0416144712666\n",
      "Current loss: 839.4952339465907\n",
      "Current loss: 838.5461333605173\n",
      "Current loss: 836.8268015913786\n",
      "Current loss: 833.657189550984\n",
      "Current loss: 827.7790053844253\n",
      "Current loss: 817.1825673971251\n",
      "Current loss: 810.8460327674126\n",
      "Current loss: 800.4515847636942\n",
      "Current loss: 785.7090136218784\n",
      "Current loss: 779.0458496546936\n",
      "Current loss: 775.6601381760621\n",
      "Current loss: 773.853028962391\n",
      "Current loss: 772.6075861540452\n",
      "Current loss: 771.9944328269348\n",
      "Current loss: 771.1965498257618\n",
      "Current loss: 769.8066875626503\n",
      "Current loss: 767.5154577504418\n",
      "Current loss: 764.4440908535807\n",
      "Current loss: 763.5464491367044\n",
      "Current loss: 762.2435326280684\n",
      "Current loss: 760.6870744678298\n",
      "Current loss: 757.9807916031032\n",
      "Current loss: 754.2355632578914\n",
      "Current loss: 753.3803470229531\n",
      "Current loss: 751.9298350462084\n",
      "Current loss: 749.4022064226041\n",
      "Current loss: 745.0615095955012\n",
      "Current loss: 737.931193628252\n",
      "Current loss: 734.5898560772185\n",
      "Current loss: 728.7855145523824\n",
      "Current loss: 722.9437665699372\n",
      "Current loss: 716.9314030531093\n",
      "Current loss: 715.0924506011427\n",
      "Current loss: 713.9788522761935\n",
      "Current loss: 712.1547265329656\n",
      "Current loss: 709.2861455569674\n",
      "Current loss: 705.8994307886345\n",
      "Current loss: 704.7304409659954\n",
      "Current loss: 702.5981662857621\n",
      "Current loss: 699.2759018852032\n",
      "Current loss: 697.4110774398281\n",
      "Current loss: 697.0761479749414\n",
      "Current loss: 696.5783252514952\n",
      "Current loss: 695.9870841755259\n",
      "Current loss: 695.5580748160704\n",
      "Current loss: 694.9618985931849\n",
      "Current loss: 694.8059306854497\n",
      "Current loss: 694.7627356304567\n",
      "Current loss: 694.6942540633117\n",
      "Current loss: 694.602104550571\n",
      "Current loss: 694.4511656183297\n",
      "Current loss: 694.1800046383402\n",
      "Current loss: 693.6761786168621\n",
      "Current loss: 692.7842434870051\n",
      "Current loss: 691.3985510030732\n",
      "Current loss: 689.7589739608644\n",
      "Current loss: 688.7741659784158\n",
      "Current loss: 687.0878751921282\n",
      "Current loss: 684.2352961242943\n",
      "Current loss: 680.3672476194295\n",
      "Current loss: 678.9702464418541\n",
      "Current loss: 677.6283710272135\n",
      "Current loss: 677.1456111554775\n",
      "Current loss: 676.8562037372482\n",
      "Current loss: 676.7635558935068\n",
      "Current loss: 676.692521990649\n",
      "Current loss: 676.5647294565338\n",
      "Current loss: 676.330931021627\n",
      "Current loss: 675.9129051957825\n",
      "Current loss: 675.2210760435779\n",
      "Current loss: 674.1856340090967\n",
      "Current loss: 672.6025255776111\n",
      "Current loss: 671.943892302884\n",
      "Current loss: 670.7402929806177\n",
      "Current loss: 668.9619781513298\n",
      "Current loss: 668.2984392751664\n",
      "Current loss: 668.1795704785121\n",
      "Current loss: 668.0273730978826\n",
      "Current loss: 667.868240381592\n",
      "Current loss: 667.6138972480993\n",
      "Current loss: 667.2448786952194\n",
      "Current loss: 666.8375114214882\n",
      "Current loss: 666.6431523939033\n",
      "Current loss: 666.2683113473145\n",
      "Current loss: 665.7494788138057\n",
      "Current loss: 665.197093385559\n",
      "Current loss: 664.9361706103512\n",
      "Current loss: 664.825994470995\n",
      "Current loss: 664.6429958174487\n",
      "Current loss: 664.3243504066769\n",
      "Current loss: 663.8137892834699\n",
      "Current loss: 663.0910751296766\n",
      "Current loss: 662.56722772211\n",
      "Current loss: 662.2354474693766\n",
      "Current loss: 662.0110182674496\n",
      "Current loss: 661.8737286486768\n",
      "Current loss: 661.7197003911864\n",
      "Current loss: 661.564539162322\n",
      "Current loss: 661.3243629087835\n",
      "Current loss: 661.0973027391224\n",
      "Current loss: 660.8508484732142\n",
      "Current loss: 660.6900659188888\n",
      "Current loss: 660.4583083084269\n",
      "Current loss: 660.226413604874\n",
      "Current loss: 660.0866143912147\n",
      "Current loss: 659.977679389726\n",
      "Current loss: 659.9442139496592\n",
      "Current loss: 659.9096563786802\n",
      "Current loss: 659.8635406084295\n",
      "Current loss: 659.7804584103213\n",
      "Current loss: 659.6888894095042\n",
      "Current loss: 659.5774300334125\n",
      "Current loss: 659.5280603243469\n",
      "Current loss: 659.4844920910123\n",
      "Current loss: 659.466080228401\n",
      "Current loss: 659.4378273674366\n",
      "Current loss: 659.3954165904568\n",
      "Current loss: 659.3430759600217\n",
      "Current loss: 659.3014976002004\n",
      "Current loss: 659.2888292466309\n",
      "Current loss: 659.2678862690675\n",
      "Current loss: 659.2562505448055\n",
      "Current loss: 659.2420393921054\n",
      "Current loss: 659.2221684499094\n",
      "Current loss: 659.2124893622594\n",
      "Current loss: 659.1983065214363\n",
      "Current loss: 659.1819805090689\n",
      "Current loss: 659.1675485759457\n",
      "Current loss: 659.1482071373066\n",
      "Current loss: 659.1360134302696\n",
      "Current loss: 659.1202896251013\n",
      "Current loss: 659.0995341119228\n",
      "Current loss: 659.0717471915808\n",
      "Current loss: 659.0553380108403\n",
      "Current loss: 659.033298630391\n",
      "Current loss: 659.013933005439\n",
      "Current loss: 658.9810444482936\n",
      "Current loss: 658.9329859804466\n",
      "Current loss: 658.8579375342831\n",
      "Current loss: 658.8198433506172\n",
      "Current loss: 658.7560746681935\n",
      "Current loss: 658.6513012914942\n",
      "Current loss: 658.6118024446021\n",
      "Current loss: 658.5440263702859\n",
      "Current loss: 658.4273576074706\n",
      "Current loss: 658.2356442713799\n",
      "Current loss: 657.9311858424185\n",
      "Current loss: 657.8689939873807\n",
      "Current loss: 657.7573221358339\n",
      "Current loss: 657.5609772878951\n",
      "Current loss: 657.2548688246832\n",
      "Current loss: 656.7714119769216\n",
      "Current loss: 656.2451881188329\n",
      "Current loss: 655.4595033270159\n",
      "Current loss: 655.2812072556782\n",
      "Current loss: 654.9644568461888\n",
      "Current loss: 654.4250629878293\n",
      "Current loss: 653.6194688365495\n",
      "Current loss: 653.0152979489292\n",
      "Current loss: 652.0655685141836\n",
      "Current loss: 651.6624445727756\n",
      "Current loss: 651.0607709434395\n",
      "Current loss: 650.0745349260901\n",
      "Current loss: 648.5066992098523\n",
      "Current loss: 647.7383219527878\n",
      "Current loss: 646.4786010022398\n",
      "Current loss: 644.5191249626743\n",
      "Current loss: 643.924969253911\n",
      "Current loss: 642.9713708439259\n",
      "Current loss: 641.4065875069429\n",
      "Current loss: 638.9735766589979\n",
      "Current loss: 638.2142803654714\n",
      "Current loss: 636.9227811919956\n",
      "Current loss: 634.7500944249575\n",
      "Current loss: 631.6996737163541\n",
      "Current loss: 627.9646830621544\n",
      "Current loss: 626.4668684355698\n",
      "Current loss: 625.3637977092719\n",
      "Current loss: 623.378907190582\n",
      "Current loss: 620.3600645668889\n",
      "Current loss: 618.1634499062415\n",
      "Current loss: 616.996783891232\n",
      "Current loss: 616.2350550918586\n",
      "Current loss: 614.777263383493\n",
      "Current loss: 612.0657119583209\n",
      "Current loss: 607.5974973233497\n",
      "Current loss: 605.5489979685964\n",
      "Current loss: 604.1967844846234\n",
      "Current loss: 602.3324068009659\n",
      "Current loss: 600.6874881885235\n",
      "Current loss: 598.1934126045395\n",
      "Current loss: 596.5420636263971\n",
      "Current loss: 594.0106538611776\n",
      "Current loss: 592.4172120781064\n",
      "Current loss: 591.7785064746864\n",
      "Current loss: 590.5861035664908\n",
      "Current loss: 588.315680301641\n",
      "Current loss: 584.1321593642383\n",
      "Current loss: 577.355564913541\n",
      "Current loss: 576.1360367258578\n",
      "Current loss: 573.9205099279178\n",
      "Current loss: 570.797325203773\n",
      "Current loss: 568.5485743580281\n",
      "Current loss: 567.3520174918988\n",
      "Current loss: 557.1832163309149\n",
      "Current loss: 547.4491737710144\n",
      "Current loss: 545.5472087263654\n",
      "Current loss: 542.8623849425632\n",
      "Current loss: 537.8634783494489\n",
      "Current loss: 529.5460496321276\n",
      "Current loss: 524.6494501579219\n",
      "Current loss: 521.7989601245538\n",
      "Current loss: 519.3326183601554\n",
      "Current loss: 516.1699471506272\n",
      "Current loss: 511.06934794993884\n",
      "Current loss: 508.05755716167374\n",
      "Current loss: 504.6137608395337\n",
      "Current loss: 501.21873555213364\n",
      "Current loss: 497.84760227664486\n",
      "Current loss: 493.32880638657105\n",
      "Current loss: 488.9501970871699\n",
      "Current loss: 485.9719888336793\n",
      "Current loss: 482.47846984548613\n",
      "Current loss: 477.1078937010483\n",
      "Current loss: 469.9461530556532\n",
      "Current loss: 458.06513437996006\n",
      "Current loss: 451.4657137217108\n",
      "Current loss: 439.94473766470986\n",
      "Current loss: 419.86989277049315\n",
      "Current loss: 384.64130351253255\n",
      "Current loss: 371.8422531811686\n",
      "Current loss: 363.5253295954573\n",
      "Current loss: 352.4095492927654\n",
      "Current loss: 343.43125539482077\n",
      "Current loss: 339.4936266953314\n",
      "Current loss: 332.66000645506875\n",
      "Current loss: 321.8886558380179\n",
      "Current loss: 317.1825771164156\n",
      "Current loss: 309.30619262545247\n",
      "Current loss: 308.4290962869246\n",
      "Current loss: 307.25215569041\n",
      "Current loss: 305.79674763188683\n",
      "Current loss: 304.4000476710529\n",
      "Current loss: 302.7174786668312\n",
      "Current loss: 301.3538669196776\n",
      "Current loss: 299.9465477609448\n",
      "Current loss: 298.8636633028302\n",
      "Current loss: 297.2178754995815\n",
      "Current loss: 295.2169071985884\n",
      "Current loss: 294.5202204294366\n",
      "Current loss: 293.385275006265\n",
      "Current loss: 291.7500238436612\n",
      "Current loss: 290.5464285186411\n",
      "Current loss: 289.620108671296\n",
      "Current loss: 289.0385016283001\n",
      "Current loss: 288.3230069018196\n",
      "Current loss: 287.85947281450245\n",
      "Current loss: 287.1399917556214\n",
      "Current loss: 286.1606644445557\n",
      "Current loss: 285.6185734944486\n",
      "Current loss: 285.3420653269505\n",
      "Current loss: 285.02292174049796\n",
      "Current loss: 284.60418211862594\n",
      "Current loss: 284.1782745532908\n",
      "Current loss: 283.8636995503195\n",
      "Current loss: 283.3738366603733\n",
      "Current loss: 283.18982381012216\n",
      "Current loss: 282.89869422369213\n",
      "Current loss: 282.43119650935284\n",
      "Current loss: 281.87096314783616\n",
      "Current loss: 281.4815532755292\n",
      "Current loss: 281.31626641083017\n",
      "Current loss: 281.02538888757897\n",
      "Current loss: 280.5391465507358\n",
      "Current loss: 280.16651170625005\n",
      "Current loss: 279.5795198682844\n",
      "Current loss: 278.76292947715774\n",
      "Current loss: 277.7439328620707\n",
      "Current loss: 277.39117289384575\n",
      "Current loss: 276.77107898434457\n",
      "Current loss: 275.7142282358377\n",
      "Current loss: 274.16752656437103\n",
      "Current loss: 273.01679253925295\n",
      "Current loss: 271.7718418376503\n",
      "Current loss: 271.31640666016665\n",
      "Current loss: 270.47684369556674\n",
      "Current loss: 269.45766283128006\n",
      "Current loss: 268.47636958418894\n",
      "Current loss: 267.5161579551137\n",
      "Current loss: 266.3282307333362\n",
      "Current loss: 265.55017925310773\n",
      "Current loss: 264.9218115621109\n",
      "Current loss: 264.1289871676444\n",
      "Current loss: 263.6364725637822\n",
      "Current loss: 262.83263267106275\n",
      "Current loss: 261.84939211843533\n",
      "Current loss: 260.75976970939314\n",
      "Current loss: 260.4951982033664\n",
      "Current loss: 260.2312863714989\n",
      "Current loss: 260.0963741469241\n",
      "Current loss: 260.078681279199\n",
      "Current loss: 260.07376390741564\n",
      "Current loss: 260.0695741041215\n",
      "Current loss: 260.06931762120126\n",
      "Current loss: 260.068820779022\n",
      "Current loss: 260.0679195644196\n",
      "Current loss: 260.0665004847303\n",
      "Current loss: 260.065050079248\n",
      "Current loss: 260.0632752503864\n",
      "Current loss: 260.062086051986\n",
      "Current loss: 260.06077483269445\n",
      "Current loss: 260.0585994148912\n",
      "Current loss: 260.05471076953353\n",
      "Current loss: 260.0475473828581\n",
      "Current loss: 260.0341842015159\n",
      "Current loss: 260.00928818480014\n",
      "Current loss: 259.9634593783529\n",
      "Current loss: 259.88098271368773\n",
      "Current loss: 259.7359890859834\n",
      "Current loss: 259.4806564238444\n",
      "Current loss: 259.0461215905161\n",
      "Current loss: 258.50225959760417\n",
      "Current loss: 257.802856323679\n",
      "Current loss: 256.8109420054271\n",
      "Current loss: 255.50698867532074\n",
      "Current loss: 255.1340130010026\n",
      "Current loss: 254.90460454209392\n",
      "Current loss: 254.56166703035646\n",
      "Current loss: 253.97949957818776\n",
      "Current loss: 253.03110598064887\n",
      "Current loss: 251.848988745133\n",
      "Current loss: 251.5799570873885\n",
      "Current loss: 251.08377933026537\n",
      "Current loss: 250.4027113077859\n",
      "Current loss: 249.39399188083374\n",
      "Current loss: 247.9446406390935\n",
      "Current loss: 247.08177133871342\n",
      "Current loss: 246.83852331514365\n",
      "Current loss: 246.36855649660197\n",
      "Current loss: 245.68348228683607\n",
      "Current loss: 244.94274713183233\n",
      "Current loss: 243.97277334377046\n",
      "Current loss: 242.61818807719644\n",
      "Current loss: 241.50347867188648\n",
      "Current loss: 240.7851595738733\n",
      "Current loss: 239.90304669719896\n",
      "Current loss: 239.31490705420993\n",
      "Current loss: 238.48853832913554\n",
      "Current loss: 237.07440843133148\n",
      "Current loss: 235.91388492354554\n",
      "Current loss: 234.11793540730332\n",
      "Current loss: 233.25968204792943\n",
      "Current loss: 232.11510772814182\n",
      "Current loss: 231.45959884264911\n",
      "Current loss: 230.36611268056498\n",
      "Current loss: 228.64027411809053\n",
      "Current loss: 226.51861989355052\n",
      "Current loss: 225.214406434252\n",
      "Current loss: 224.44899303544284\n",
      "Current loss: 223.16070435376258\n",
      "Current loss: 221.33252711735966\n",
      "Current loss: 220.52890924542118\n",
      "Current loss: 219.50305753038657\n",
      "Current loss: 218.7728439844572\n",
      "Current loss: 217.67035357165184\n",
      "Current loss: 217.12742453354056\n",
      "Current loss: 216.22810638015082\n",
      "Current loss: 214.69484148177807\n",
      "Current loss: 212.23418790613343\n",
      "Current loss: 211.37449193920907\n",
      "Current loss: 209.96236431786872\n",
      "Current loss: 208.3543734336426\n",
      "Current loss: 207.95988067709501\n",
      "Current loss: 207.2733639081293\n",
      "Current loss: 206.09262453527418\n",
      "Current loss: 204.18057718425283\n",
      "Current loss: 201.84045917703975\n",
      "Current loss: 199.1544708220529\n",
      "Current loss: 197.93353980895418\n",
      "Current loss: 196.73182643490765\n",
      "Current loss: 194.7634087026214\n",
      "Current loss: 192.01774671681602\n",
      "Current loss: 189.48021506492884\n",
      "Current loss: 187.72590205277362\n",
      "Current loss: 186.3216957960942\n",
      "Current loss: 184.5590716556289\n",
      "Current loss: 183.11329925949516\n",
      "Current loss: 181.46176607020095\n",
      "Current loss: 179.30276063578577\n",
      "Current loss: 177.33370558402356\n",
      "Current loss: 174.23930494862478\n",
      "Current loss: 170.5042677981908\n",
      "Current loss: 167.7603461142742\n",
      "Current loss: 165.6817834550731\n",
      "Current loss: 164.4813923513882\n",
      "Current loss: 163.22636216762982\n",
      "Current loss: 161.26781258176845\n",
      "Current loss: 159.34766083400007\n",
      "Current loss: 157.34284392926205\n",
      "Current loss: 154.55144003607305\n",
      "Current loss: 149.81020706036213\n",
      "Current loss: 147.70468665869558\n",
      "Current loss: 144.69304550747873\n",
      "Current loss: 143.97137574612648\n",
      "Current loss: 142.97829797226336\n",
      "Current loss: 141.74541812653928\n",
      "Current loss: 139.58942640683634\n",
      "Current loss: 135.58531427895798\n",
      "Current loss: 129.381692925831\n",
      "Current loss: 125.71222900345724\n",
      "Current loss: 120.1852423876865\n",
      "Current loss: 118.28088716614009\n",
      "Current loss: 114.78519453738262\n",
      "Current loss: 113.88178153890202\n",
      "Current loss: 112.15157601094785\n",
      "Current loss: 109.05434780673566\n",
      "Current loss: 103.94762683196869\n",
      "Current loss: 96.99769022328432\n",
      "Current loss: 94.7855771111157\n",
      "Current loss: 92.05224814455283\n",
      "Current loss: 87.8083347162628\n",
      "Current loss: 81.26041027063864\n",
      "Current loss: 73.25543670053392\n",
      "Current loss: 67.66061032182529\n",
      "Current loss: 60.333127993708565\n",
      "Current loss: 58.659765936475104\n",
      "Current loss: 57.99964216525296\n",
      "Current loss: 57.673748618136386\n",
      "Current loss: 57.33275262171131\n",
      "Current loss: 56.932637955158256\n",
      "Current loss: 56.84767604678266\n",
      "Current loss: 56.83984573481098\n",
      "Current loss: 56.8355042441093\n",
      "Current loss: 56.83078211558052\n",
      "Current loss: 56.82239162106118\n",
      "Current loss: 56.80589785388066\n",
      "Current loss: 56.77587052416078\n",
      "Current loss: 56.73986144125559\n",
      "Current loss: 56.71501270265239\n",
      "Current loss: 56.67683912261335\n",
      "Current loss: 56.64596731786124\n",
      "Current loss: 56.584574328824054\n",
      "Current loss: 56.453722503490894\n",
      "Current loss: 56.18350212047963\n",
      "Current loss: 55.71155545399984\n",
      "Current loss: 55.00024642482469\n",
      "Current loss: 54.36823877725399\n",
      "Current loss: 53.308021218568776\n",
      "Current loss: 52.66625855523808\n",
      "Current loss: 51.87705879536327\n",
      "Current loss: 51.59922348183185\n",
      "Current loss: 51.235410134273366\n",
      "Current loss: 51.022313917726834\n",
      "Current loss: 50.64881149232407\n",
      "Current loss: 50.04311236168204\n",
      "Current loss: 49.47208000543281\n",
      "Current loss: 48.747230619101956\n",
      "Current loss: 48.259549150765196\n",
      "Current loss: 47.925667275291076\n",
      "Current loss: 47.46954731789974\n",
      "Current loss: 47.12671512127874\n",
      "Current loss: 46.739090176082215\n",
      "Current loss: 46.377708438370924\n",
      "Current loss: 45.96510591690905\n",
      "Current loss: 45.64754496658391\n",
      "Current loss: 45.449808422299824\n",
      "Current loss: 45.24274706762773\n",
      "Current loss: 45.01209494166091\n",
      "Current loss: 44.671284203851656\n",
      "Current loss: 44.47955556128955\n",
      "Current loss: 44.20827448108481\n",
      "Current loss: 43.90600875966484\n",
      "Current loss: 43.77045754300491\n",
      "Current loss: 43.64320698872481\n",
      "Current loss: 43.54989938958841\n",
      "Current loss: 43.399366162886075\n",
      "Current loss: 43.18496068003269\n",
      "Current loss: 42.789505957389245\n",
      "Current loss: 42.17406097596853\n",
      "Current loss: 41.762523658972604\n",
      "Current loss: 41.32036351347324\n",
      "Current loss: 40.58612336043599\n",
      "Current loss: 39.379774034019746\n",
      "Current loss: 37.7790052115055\n",
      "Current loss: 36.96425952760832\n",
      "Current loss: 36.39649765597645\n",
      "Current loss: 36.282528690603186\n",
      "Current loss: 36.1818230583814\n",
      "Current loss: 36.134855981220504\n",
      "Current loss: 36.0855383254362\n",
      "Current loss: 36.05436225334435\n",
      "Current loss: 36.0457235455686\n",
      "Current loss: 36.02384454713389\n",
      "Current loss: 35.971679862853485\n",
      "Current loss: 35.85426220630783\n",
      "Current loss: 35.669468858123054\n",
      "Current loss: 35.415708540605195\n",
      "Current loss: 35.198471133002904\n",
      "Current loss: 34.82099779048177\n",
      "Current loss: 34.462760271561045\n",
      "Current loss: 34.17233667681943\n",
      "Current loss: 34.07531257026047\n",
      "Current loss: 34.02381396996507\n",
      "Current loss: 34.01257550695443\n",
      "Current loss: 33.994708040123726\n",
      "Current loss: 33.96321439204803\n",
      "Current loss: 33.90662644447933\n",
      "Current loss: 33.81337588138456\n",
      "Current loss: 33.67987096490717\n",
      "Current loss: 33.56774850989075\n",
      "Current loss: 33.4123298572885\n",
      "Current loss: 33.264446261336126\n",
      "Current loss: 33.17734576529133\n",
      "Current loss: 33.136888677206464\n",
      "Current loss: 33.08181711731616\n",
      "Current loss: 32.982707126748984\n",
      "Current loss: 32.7978430020832\n",
      "Current loss: 32.58191519833435\n",
      "Current loss: 32.56160890961127\n",
      "Current loss: 32.52503828327012\n",
      "Current loss: 32.4523775305355\n",
      "Current loss: 32.311017751898866\n",
      "Current loss: 32.064930309018365\n",
      "Current loss: 32.01484892386252\n",
      "Current loss: 31.91721399037047\n",
      "Current loss: 31.843899827821048\n",
      "Current loss: 31.803999509686854\n",
      "Current loss: 31.729575860251174\n",
      "Current loss: 31.577609034308562\n",
      "Current loss: 31.301104233463683\n",
      "Current loss: 31.16323617357151\n",
      "Current loss: 30.98684708075787\n",
      "Current loss: 30.954081435630975\n",
      "Current loss: 30.902081527838252\n",
      "Current loss: 30.821626959383206\n",
      "Current loss: 30.75192876627591\n",
      "Current loss: 30.681697417185948\n",
      "Current loss: 30.55516223112416\n",
      "Current loss: 30.376497144107887\n",
      "Current loss: 30.284167891225906\n",
      "Current loss: 30.136626711546963\n",
      "Current loss: 29.992881733716484\n",
      "Current loss: 29.954471694081263\n",
      "Current loss: 29.881006801255953\n",
      "Current loss: 29.810176692111586\n",
      "Current loss: 29.73339238208299\n",
      "Current loss: 29.594666247419994\n",
      "Current loss: 29.371251514689973\n",
      "Current loss: 29.24423565751995\n",
      "Current loss: 29.024321559043315\n",
      "Current loss: 28.89234009689747\n",
      "Current loss: 28.865632183944776\n",
      "Current loss: 28.82821481697317\n",
      "Current loss: 28.795129467974768\n",
      "Current loss: 28.76769882736371\n",
      "Current loss: 28.712607108727376\n",
      "Current loss: 28.61055636911023\n",
      "Current loss: 28.475948344861717\n",
      "Current loss: 28.39166724672432\n",
      "Current loss: 28.244390412617772\n",
      "Current loss: 28.06017292871749\n",
      "Current loss: 27.9939699334379\n",
      "Current loss: 27.94408264894881\n",
      "Current loss: 27.850037364730106\n",
      "Current loss: 27.749464160361256\n",
      "Current loss: 27.709398637049894\n",
      "Current loss: 27.652315255918623\n",
      "Current loss: 27.55999688940123\n",
      "Current loss: 27.40038853240104\n",
      "Current loss: 27.187941294743894\n",
      "Current loss: 26.947308237275294\n",
      "Current loss: 26.842327248530403\n",
      "Current loss: 26.77887805289723\n",
      "Current loss: 26.70714260338837\n",
      "Current loss: 26.69020242657262\n",
      "Current loss: 26.674690155641954\n",
      "Current loss: 26.64305174554617\n",
      "Current loss: 26.58289413061931\n",
      "Current loss: 26.46730056687818\n",
      "Current loss: 26.316898404931674\n",
      "Current loss: 26.2513288820706\n",
      "Current loss: 26.15944997730079\n",
      "Current loss: 26.08437312227601\n",
      "Current loss: 26.0425806440427\n",
      "Current loss: 25.9991217848597\n",
      "Current loss: 25.945327500756754\n",
      "Current loss: 25.877122868006072\n",
      "Current loss: 25.78546750773927\n",
      "Current loss: 25.72323868017713\n",
      "Current loss: 25.62269847006843\n",
      "Current loss: 25.445434181295965\n",
      "Current loss: 25.260009516149175\n",
      "Current loss: 25.174265111958967\n",
      "Current loss: 25.08630953104414\n",
      "Current loss: 25.078919042893517\n",
      "Current loss: 25.0645795916269\n",
      "Current loss: 25.038759599444546\n",
      "Current loss: 25.018165193623553\n",
      "Current loss: 24.992286975484863\n",
      "Current loss: 24.938337973221273\n",
      "Current loss: 24.82934123256403\n",
      "Current loss: 24.649108507583666\n",
      "Current loss: 24.6166800841388\n",
      "Current loss: 24.551916206238776\n",
      "Current loss: 24.465514611461828\n",
      "Current loss: 24.411760328296495\n",
      "Current loss: 24.33379616750418\n",
      "Current loss: 24.287106086666764\n",
      "Current loss: 24.21700493561756\n",
      "Current loss: 24.15335906988237\n",
      "Current loss: 24.0902556084442\n",
      "Current loss: 24.077123387162963\n",
      "Current loss: 24.051541880021457\n",
      "Current loss: 24.00077617032972\n",
      "Current loss: 23.952902730905937\n",
      "Current loss: 23.908291070790355\n",
      "Current loss: 23.84838013255434\n",
      "Current loss: 23.73116893004226\n",
      "Current loss: 23.645660746308437\n",
      "Current loss: 23.50671239720151\n",
      "Current loss: 23.3643759973691\n",
      "Current loss: 23.317465422821375\n",
      "Current loss: 23.28499499601919\n",
      "Current loss: 23.274692133482368\n",
      "Current loss: 23.267561987822955\n",
      "Current loss: 23.251738450614535\n",
      "Current loss: 23.219759286109806\n",
      "Current loss: 23.172587280390175\n",
      "Current loss: 23.116885799653588\n",
      "Current loss: 23.04464337455379\n",
      "Current loss: 22.977152470241368\n",
      "Current loss: 22.94439016123837\n",
      "Current loss: 22.908497617500878\n",
      "Current loss: 22.87286890503764\n",
      "Current loss: 22.834645528477786\n",
      "Current loss: 22.77486074171706\n",
      "Current loss: 22.67202980979116\n",
      "Current loss: 22.538565818785766\n",
      "Current loss: 22.523792666354815\n",
      "Current loss: 22.495257873656428\n",
      "Current loss: 22.459933415652863\n",
      "Current loss: 22.422495453392038\n",
      "Current loss: 22.35515783656882\n",
      "Current loss: 22.27394530760267\n",
      "Current loss: 22.177346531293484\n",
      "Current loss: 22.12398629347359\n",
      "Current loss: 22.094195366901047\n",
      "Current loss: 22.06165590465065\n",
      "Current loss: 22.010932839482752\n",
      "Current loss: 21.93333363500661\n",
      "Current loss: 21.813155349651996\n",
      "Current loss: 21.760088386310112\n",
      "Current loss: 21.693809454829452\n",
      "Current loss: 21.65587429875546\n",
      "Current loss: 21.608246194997047\n",
      "Current loss: 21.530243185912802\n",
      "Current loss: 21.438173685332835\n",
      "Current loss: 21.34165393033687\n",
      "Current loss: 21.28310646519132\n",
      "Current loss: 21.201657978148425\n",
      "Current loss: 21.138109519552213\n",
      "Current loss: 21.05947044242545\n",
      "Current loss: 21.044177544685724\n",
      "Current loss: 21.014401377386157\n",
      "Current loss: 20.958577997951146\n",
      "Current loss: 20.8819300642009\n",
      "Current loss: 20.789263542149946\n",
      "Current loss: 20.62177091084662\n",
      "Current loss: 20.369023437516134\n",
      "Current loss: 20.161214460450456\n",
      "Current loss: 20.03904437969112\n",
      "Current loss: 19.93721796441509\n",
      "Current loss: 19.82204220419947\n",
      "Current loss: 19.78510265563828\n",
      "Current loss: 19.72314277247983\n",
      "Current loss: 19.637261898494803\n",
      "Current loss: 19.578789275484272\n",
      "Current loss: 19.532382175398162\n",
      "Current loss: 19.50319632168134\n",
      "Current loss: 19.486051140091778\n",
      "Current loss: 19.456783257920176\n",
      "Current loss: 19.405753690948796\n",
      "Current loss: 19.318534946017028\n",
      "Current loss: 19.186564304497654\n",
      "Current loss: 19.081766493231164\n",
      "Current loss: 19.04834226947385\n",
      "Current loss: 18.98667815070238\n",
      "Current loss: 18.888160667621023\n",
      "Current loss: 18.760601150447528\n",
      "Current loss: 18.605306773376437\n",
      "Current loss: 18.517764253496974\n",
      "Current loss: 18.418386246656556\n",
      "Current loss: 18.26327984594015\n",
      "Current loss: 18.12418506597077\n",
      "Current loss: 18.095184805993796\n",
      "Current loss: 18.039356186087076\n",
      "Current loss: 17.997665670366416\n",
      "Current loss: 17.971375295144362\n",
      "Current loss: 17.937096431371515\n",
      "Current loss: 17.909828321145234\n",
      "Current loss: 17.872784033491715\n",
      "Current loss: 17.861183253684082\n",
      "Current loss: 17.85533565502146\n",
      "Current loss: 17.85183940554105\n",
      "Current loss: 17.850848279378692\n",
      "Current loss: 17.850562817537536\n",
      "Current loss: 17.850226955562274\n",
      "Current loss: 17.850172178195635\n",
      "Current loss: 17.850053941285026\n",
      "Current loss: 17.85001264815547\n",
      "Current loss: 17.849972037698585\n",
      "Current loss: 17.84995740748414\n",
      "Current loss: 17.849920529293815\n",
      "Current loss: 17.849815119004194\n",
      "Current loss: 17.849694569696897\n",
      "Current loss: 17.849415352356974\n",
      "Current loss: 17.848783603353272\n",
      "Current loss: 17.847448920559494\n",
      "Current loss: 17.844720438523893\n",
      "Current loss: 17.839616611796977\n",
      "Current loss: 17.830740834670216\n",
      "Current loss: 17.815348058538092\n",
      "Current loss: 17.783447982814128\n",
      "Current loss: 17.72120060129661\n",
      "Current loss: 17.60422880047394\n",
      "Current loss: 17.420769702541676\n",
      "Current loss: 17.092930686212423\n",
      "Current loss: 16.695345334911426\n",
      "Current loss: 16.561778690747342\n",
      "Current loss: 16.419636437138116\n",
      "Current loss: 16.203332422367392\n",
      "Current loss: 16.08971516406752\n",
      "Current loss: 15.936607216073917\n",
      "Current loss: 15.75535808203877\n",
      "Current loss: 15.621474682704584\n",
      "Current loss: 15.459638720237056\n",
      "Current loss: 15.366572994666376\n",
      "Current loss: 15.208014950612483\n",
      "Current loss: 15.033460606213211\n",
      "Current loss: 14.816066354370678\n",
      "Current loss: 14.607007633856442\n",
      "Current loss: 14.403578175848603\n",
      "Current loss: 14.221245336401081\n",
      "Current loss: 14.104682270450994\n",
      "Current loss: 13.919691648644568\n",
      "Current loss: 13.676083263380214\n",
      "Current loss: 13.511453132270054\n",
      "Current loss: 13.382902757254824\n",
      "Current loss: 13.204940042402527\n",
      "Current loss: 12.95365955854708\n",
      "Current loss: 12.765937915261807\n",
      "Current loss: 12.52861338146467\n",
      "Current loss: 12.36885202470691\n",
      "Current loss: 12.20733169967328\n",
      "Current loss: 12.052903403107443\n",
      "Current loss: 11.833278491680296\n",
      "Current loss: 11.717845053064922\n",
      "Current loss: 11.552039801114319\n",
      "Current loss: 11.338218292253705\n",
      "Current loss: 11.183614352425796\n",
      "Current loss: 10.993852279205228\n",
      "Current loss: 10.781169762448227\n",
      "Current loss: 10.597403406172129\n",
      "Current loss: 10.431453504888749\n",
      "Current loss: 10.261992546120013\n",
      "Current loss: 10.078449202149908\n",
      "Current loss: 9.883862173374755\n",
      "Current loss: 9.789582114727361\n",
      "Current loss: 9.64054005517845\n",
      "Current loss: 9.41563134754851\n",
      "Current loss: 9.217239466486346\n",
      "Current loss: 8.954846114699455\n",
      "Current loss: 8.873677355566501\n",
      "Current loss: 8.729952649625515\n",
      "Current loss: 8.497428920606197\n",
      "Current loss: 8.412787029191103\n",
      "Current loss: 8.260583468914346\n",
      "Current loss: 8.14576292646311\n",
      "Current loss: 7.951544952326556\n",
      "Current loss: 7.84407186732316\n",
      "Current loss: 7.664698657700303\n",
      "Current loss: 7.465155120974739\n",
      "Current loss: 7.306170174498948\n",
      "Current loss: 7.010632451927904\n",
      "Current loss: 6.986992001136737\n",
      "Current loss: 6.947848076557046\n",
      "Current loss: 6.875255706297628\n",
      "Current loss: 6.7509852290359325\n",
      "Current loss: 6.5603378775611585\n",
      "Current loss: 6.339350289163062\n",
      "Current loss: 6.260371350345318\n",
      "Current loss: 6.1860243413384906\n",
      "Current loss: 6.077846422427108\n",
      "Current loss: 5.9257256348399165\n",
      "Current loss: 5.845912953055666\n",
      "Current loss: 5.718318229480347\n",
      "Current loss: 5.566180900287526\n",
      "Current loss: 5.506776224587389\n",
      "Current loss: 5.404317089796429\n",
      "Current loss: 5.235155876607721\n",
      "Current loss: 5.140644119580368\n",
      "Current loss: 4.989725533231137\n",
      "Current loss: 4.856643762751517\n",
      "Current loss: 4.754006324629094\n",
      "Current loss: 4.715991147094159\n",
      "Current loss: 4.643459395080918\n",
      "Current loss: 4.538148297852206\n",
      "Current loss: 4.372989505633352\n",
      "Current loss: 4.2541616932719855\n",
      "Current loss: 4.117250405977506\n",
      "Current loss: 4.048138368030877\n",
      "Current loss: 3.9632685568759483\n",
      "Current loss: 3.8642234649028553\n",
      "Current loss: 3.7356128226794225\n",
      "Current loss: 3.567881792984167\n",
      "Current loss: 3.4544966537378476\n",
      "Current loss: 3.366087818289327\n",
      "Current loss: 3.283339434679551\n",
      "Current loss: 3.2346363556883952\n",
      "Current loss: 3.1537731503622832\n",
      "Current loss: 3.0740458479931796\n",
      "Current loss: 2.9909942449210565\n",
      "Current loss: 2.8979900999875854\n",
      "Current loss: 2.772673636410372\n",
      "Current loss: 2.727499157004498\n",
      "Current loss: 2.652176583923622\n",
      "Current loss: 2.562803829863647\n",
      "Current loss: 2.4353242752743967\n",
      "Current loss: 2.237572159872092\n",
      "Current loss: 1.9961477763731377\n",
      "Current loss: 1.8862981493713897\n",
      "Current loss: 1.6941432496543682\n",
      "Current loss: 1.6032368610006889\n",
      "Current loss: 1.5159332006273039\n",
      "Current loss: 1.4558414561786668\n",
      "Current loss: 1.3875272114541317\n",
      "Current loss: 1.3173931388649913\n",
      "Current loss: 1.2849980864278399\n",
      "Current loss: 1.2702205447938733\n",
      "Current loss: 1.2614198721712415\n",
      "Current loss: 1.2588612403670187\n",
      "Current loss: 1.2578554858907993\n",
      "Current loss: 1.256507208387895\n",
      "Current loss: 1.2558741212467088\n",
      "Current loss: 1.2552454616063407\n",
      "Current loss: 1.2547788796388837\n",
      "Current loss: 1.2546469926090293\n",
      "Current loss: 1.2545064821589575\n",
      "Current loss: 1.2536715163004417\n",
      "Current loss: 1.252748916665753\n",
      "Current loss: 1.2515207758364912\n",
      "Current loss: 1.2497994291928238\n",
      "Current loss: 1.2473480970923283\n",
      "Current loss: 1.2452220949003674\n",
      "Current loss: 1.2436025053847681\n",
      "Current loss: 1.2419395180713244\n",
      "Current loss: 1.2417249519463673\n",
      "Current loss: 1.2417249206562035\n",
      "Current loss: 1.2417230937340358\n",
      "Current loss: 1.2417064284909527\n",
      "Current loss: 1.2416925407094759\n",
      "Current loss: 1.2416821447979747\n",
      "Current loss: 1.2416634493836465\n",
      "Current loss: 1.2415228783167505\n",
      "Current loss: 1.2413153352999353\n",
      "Current loss: 1.2410016255050453\n",
      "Current loss: 1.2404794550427622\n",
      "Current loss: 1.2395935639514304\n",
      "Current loss: 1.2380907893173654\n",
      "Current loss: 1.235532778195367\n",
      "Current loss: 1.2312478403688374\n",
      "Current loss: 1.2247366603125696\n",
      "Current loss: 1.2145216988695868\n",
      "Current loss: 1.1981948647874372\n",
      "Current loss: 1.1903502892123068\n",
      "Current loss: 1.1840282300252825\n",
      "Current loss: 1.1830264519915477\n",
      "Current loss: 1.1822449020633776\n",
      "Current loss: 1.1812030080749687\n",
      "Current loss: 1.1794677813564796\n",
      "Current loss: 1.1776139559448742\n",
      "Current loss: 1.1755282662324846\n",
      "Current loss: 1.1719436197395867\n",
      "Current loss: 1.1662647922188896\n",
      "Current loss: 1.157255586154854\n",
      "Current loss: 1.143121760429241\n",
      "Current loss: 1.1361236080181996\n",
      "Current loss: 1.1245001181080059\n",
      "Current loss: 1.1051531124393539\n",
      "Current loss: 1.0969911671167576\n",
      "Current loss: 1.0821090327795975\n",
      "Current loss: 1.0553826004054627\n",
      "Current loss: 1.0092871918986897\n",
      "Current loss: 0.9337168517219964\n",
      "Current loss: 0.8954396879362146\n",
      "Current loss: 0.8350644645338579\n",
      "Current loss: 0.7387948149041381\n",
      "Current loss: 0.6656676657115055\n",
      "Current loss: 0.5320237258482888\n",
      "Current loss: 0.5096772244934095\n",
      "Current loss: 0.4776617988678103\n",
      "Current loss: 0.4182259396313139\n",
      "Current loss: 0.3169621393872456\n",
      "Current loss: 0.25884133492778044\n",
      "Current loss: 0.22359171006631642\n",
      "Current loss: 0.17764878514255028\n",
      "Current loss: 0.1273289920496547\n",
      "Current loss: 0.06606530292798614\n",
      "Current loss: 0.05788094871409705\n",
      "Current loss: 0.04905287951475699\n",
      "Current loss: 0.03551346664703512\n",
      "Current loss: 0.033885334557891\n",
      "Current loss: 0.02879815453055429\n",
      "Current loss: 0.023386173184978976\n",
      "Current loss: 0.014215728122799375\n",
      "Current loss: 0.01397474233645355\n",
      "Current loss: 0.013974742334215275\n",
      "Current loss: 0.013957050684202605\n",
      "Current loss: 0.013932236417944565\n",
      "Current loss: 0.013899874552755011\n",
      "Current loss: 0.01386101770804053\n",
      "Current loss: 0.013815143514256178\n",
      "Current loss: 0.013761523804641681\n",
      "Current loss: 0.013697803185306919\n",
      "Current loss: 0.013621173146233602\n",
      "Current loss: 0.01352879128655538\n",
      "Current loss: 0.013422046660281597\n",
      "Current loss: 0.013303130159679653\n",
      "Current loss: 0.013177446325894071\n",
      "Current loss: 0.01305448860042742\n",
      "Current loss: 0.012948394955736237\n",
      "Current loss: 0.012926501701092791\n",
      "Current loss: 0.012926406294639703\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# theta = np.random.uniform(0, 2 * np.pi, size=(2,2))\n",
    "\n",
    "shape = qml.StronglyEntanglingLayers.shape(n_layers=num_layers, n_wires=num_qubits)\n",
    "print(shape)\n",
    "theta = np.random.random(size=shape)\n",
    "\n",
    "w = np.zeros(2)\n",
    "\n",
    "# initial_params = np.concatenate([w, theta.flatten()])  # Flatten theta to a 1D array\n",
    "initial_params = np.concatenate([w, theta.flatten()])  # Flatten theta to a 1D array\n",
    "\n",
    "result = minimize(cost, initial_params, callback=callbackF, method='BFGS', options={'gtol': 1E-2})\n",
    "# Extract the optimized parameters\n",
    "w, theta_flat = result.x[:2], result.x[2:]\n",
    "theta = theta_flat.reshape(theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-24.84383684   9.88788625]\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights tensor must have second dimension of length 2; got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Git repo\\DE_Solver_PowerSeriers\\Design2\\learn_design6 copy.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Calculate corresponding y values for both functions\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y1 \u001b[39m=\u001b[39m [f(x,theta[\u001b[39m0\u001b[39;49m],w) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m t]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y2 \u001b[39m=\u001b[39m [f_test(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m t]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Create the plot\u001b[39;00m\n",
      "\u001b[1;32mc:\\Git repo\\DE_Solver_PowerSeriers\\Design2\\learn_design6 copy.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Calculate corresponding y values for both functions\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y1 \u001b[39m=\u001b[39m [f(x,theta[\u001b[39m0\u001b[39;49m],w) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m t]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y2 \u001b[39m=\u001b[39m [f_test(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m t]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Create the plot\u001b[39;00m\n",
      "\u001b[1;32mc:\\Git repo\\DE_Solver_PowerSeriers\\Design2\\learn_design6 copy.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x,theta,w):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     encoded_x,norm_x \u001b[39m=\u001b[39m encoder(x)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     f_val \u001b[39m=\u001b[39m hepler(encoded_x,theta)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# rescale_coef = abs(w[0])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     rescale_coef \u001b[39m=\u001b[39m w[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\acade\\anaconda3\\envs\\q1\\Lib\\site-packages\\pennylane\\qnode.py:936\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m             kwargs[\u001b[39m\"\u001b[39m\u001b[39mshots\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[39m# construct the tape\u001b[39;00m\n\u001b[1;32m--> 936\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstruct(args, kwargs)\n\u001b[0;32m    938\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    939\u001b[0m using_custom_cache \u001b[39m=\u001b[39m (\n\u001b[0;32m    940\u001b[0m     \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__getitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    941\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__setitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__delitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    943\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\acade\\anaconda3\\envs\\q1\\Lib\\site-packages\\pennylane\\qnode.py:827\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[39mif\u001b[39;00m old_interface \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    825\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterface \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mget_interface(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m--> 827\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape \u001b[39m=\u001b[39m make_qscript(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, shots)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    828\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39m_qfunc_output\n\u001b[0;32m    830\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mget_parameters(trainable_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\acade\\anaconda3\\envs\\q1\\Lib\\site-packages\\pennylane\\tape\\qscript.py:1482\u001b[0m, in \u001b[0;36mmake_qscript.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1481\u001b[0m     \u001b[39mwith\u001b[39;00m AnnotatedQueue() \u001b[39mas\u001b[39;00m q:\n\u001b[1;32m-> 1482\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1484\u001b[0m     qscript \u001b[39m=\u001b[39m QuantumScript\u001b[39m.\u001b[39mfrom_queue(q, shots)\n\u001b[0;32m   1485\u001b[0m     qscript\u001b[39m.\u001b[39m_qfunc_output \u001b[39m=\u001b[39m result\n",
      "\u001b[1;32mc:\\Git repo\\DE_Solver_PowerSeriers\\Design2\\learn_design6 copy.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qml\u001b[39m.\u001b[39mQubitStateVector(encoded_x,wires\u001b[39m=\u001b[39m[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_qubits)])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# qml.RY(wires=0,phi=theta[0])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# qml.RX(wires=0,phi=theta[1])\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m qml\u001b[39m.\u001b[39;49mStronglyEntanglingLayers(weights\u001b[39m=\u001b[39;49mtheta, wires\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m(num_qubits))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git%20repo/DE_Solver_PowerSeriers/Design2/learn_design6%20copy.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m qml\u001b[39m.\u001b[39mprobs(wires\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\acade\\anaconda3\\envs\\q1\\Lib\\site-packages\\pennylane\\templates\\layers\\strongly_entangling.py:139\u001b[0m, in \u001b[0;36mStronglyEntanglingLayers.__init__\u001b[1;34m(self, weights, wires, ranges, imprimitive, do_queue, id)\u001b[0m\n\u001b[0;32m    136\u001b[0m shape \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mshape(weights)[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(wires):\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights tensor must have second dimension of length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(wires)\u001b[39m}\u001b[39;00m\u001b[39m; got \u001b[39m\u001b[39m{\u001b[39;00mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m2\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    145\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights tensor must have third dimension of length 3; got \u001b[39m\u001b[39m{\u001b[39;00mshape[\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Weights tensor must have second dimension of length 2; got 3"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate corresponding y values for both functions\n",
    "y1 = [f(x,theta[0],w) for x in t]\n",
    "y2 = [f_test(x) for x in t]\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.scatter(t, y1, label='Quantum f(x)', linewidth=2, color='b')  # Plot the first function\n",
    "plt.plot(t, y2, label='Analytic f(x)', linewidth=2, color='r')  # Plot the second function\n",
    "plt.xlabel('x')  # X-axis label\n",
    "plt.ylabel('y')  # Y-axis label\n",
    "# plt.title('Plot of Two Functions')  # Title of the plot\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  # Add a grid\n",
    "plt.legend()  # Show the legend\n",
    "plt.axhline(0, color='black', linewidth=0.5)  # Add horizontal line at y=0\n",
    "plt.axvline(0, color='black', linewidth=0.5)  # Add vertical line at x=0\n",
    "# plt.ylim([-5, 10])  # Set y-axis limits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
